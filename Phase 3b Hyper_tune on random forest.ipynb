{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184e32a-8e03-4881-af14-c1c8f4a60681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f03adc-f797-4dde-8fe5-6c467df666e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df = pd.read_csv('IMDB-Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84292c2-3a0e-4e8a-82fa-4054b0bc5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "df.drop(columns = ['Resenhas'],inplace=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e5093-b186-4a4f-b80e-bc123cdda201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stopwords = [\"would\",\"shall\",\"could\",\"might\"]\n",
    "stop_words.extend(new_stopwords)\n",
    "stop_words.remove(\"not\")\n",
    "stop_words=set(stop_words)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784a486-4976-4e2a-b267-faf2c6783fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_character(content):\n",
    "    return re.sub('\\W+',' ', content )\n",
    "\n",
    "def remove_url(content):\n",
    "    return re.sub(r'http\\S+', '', content)\n",
    "\n",
    "def remove_stopwords(content):\n",
    "    clean_data = []\n",
    "    for i in content.split():\n",
    "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
    "            clean_data.append(i.strip().lower())\n",
    "    return \" \".join(clean_data)\n",
    "\n",
    "def contraction_expansion(content):\n",
    "    content = re.sub(r\"won\\'t\", \"would not\", content)\n",
    "    content = re.sub(r\"can\\'t\", \"can not\", content)\n",
    "    content = re.sub(r\"don\\'t\", \"do not\", content)\n",
    "    content = re.sub(r\"shouldn\\'t\", \"should not\", content)\n",
    "    content = re.sub(r\"needn\\'t\", \"need not\", content)\n",
    "    content = re.sub(r\"hasn\\'t\", \"has not\", content)\n",
    "    content = re.sub(r\"haven\\'t\", \"have not\", content)\n",
    "    content = re.sub(r\"weren\\'t\", \"were not\", content)\n",
    "    content = re.sub(r\"mightn\\'t\", \"might not\", content)\n",
    "    content = re.sub(r\"didn\\'t\", \"did not\", content)\n",
    "    content = re.sub(r\"n\\'t\", \" not\", content)\n",
    "    '''content = re.sub(r\"\\'re\", \" are\", content)\n",
    "    content = re.sub(r\"\\'s\", \" is\", content)\n",
    "    content = re.sub(r\"\\'d\", \" would\", content)\n",
    "    content = re.sub(r\"\\'ll\", \" will\", content)\n",
    "    content = re.sub(r\"\\'t\", \" not\", content)\n",
    "    content = re.sub(r\"\\'ve\", \" have\", content)\n",
    "    content = re.sub(r\"\\'m\", \" am\", content)'''\n",
    "    return content\n",
    "\n",
    "def data_cleaning(content):\n",
    "    content = contraction_expansion(content)\n",
    "    content = remove_special_character(content)\n",
    "    content = remove_url(content)\n",
    "    \n",
    "    content = remove_stopwords(content)    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b317f-7871-4b69-9360-1bb32de420d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pd.options.display.max_colwidth = 1000\n",
    "df['Reviews_clean']=df['Reviews'].apply(data_cleaning)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b92f3b-a1fb-4bff-821e-6a5f4106e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Ratings'].apply(lambda x: '1' if x >= 7 else ('0' if x<=4 else '2'))\n",
    "df=df[df.Label<'2']\n",
    "data=df[['Reviews_clean','Reviews','Ratings','Label']]\n",
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a944f4-01f0-41c6-a253-c48e0a1a18f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5555714b-c5fc-4432-afc1-a7f7c751ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wordnetlemma = WordNetLemmatizer()\n",
    "    def __call__(self, reviews):\n",
    "        return [self.wordnetlemma.lemmatize(word) for word in word_tokenize(reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d4231-8e22-4213-8430-50695889fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "#countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3), min_df=10,max_features=5000)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3),min_df=10,max_features=10000)\n",
    "#x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "#x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5c0ee-b1e1-4023-bb8d-21611767baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 5000\n",
    "Number = 1\n",
    "featureselection = PrettyTable([\"Unigram\", \"Bigram\",\"Trigram\"])\n",
    "for category in train['Label'].unique():\n",
    "    features_chi2 = chi2(x_train_tfidf, train['Label'] == category)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidfvect.get_feature_names_out())[indices]\n",
    "    unigrams = [x for x in feature_names if len(x.split(' ')) == 1]\n",
    "    bigrams = [x for x in feature_names if len(x.split(' ')) == 2]\n",
    "    trigrams = [x for x in feature_names if len(x.split(' ')) == 3]\n",
    "    print(\"%s. %s :\" % (Number,category))\n",
    "    print(\"\\t# Unigrams :\\n\\t. %s\" %('\\n\\t. '.join(unigrams[-N:])))\n",
    "    print(\"\\t# Bigrams :\\n\\t. %s\" %('\\n\\t. '.join(bigrams[-N:])))\n",
    "    print(\"\\t# Trigrams :\\n\\t. %s\" %('\\n\\t. '.join(trigrams[-N:])))\n",
    "    Number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2d3c0-f20c-4e77-a6b7-9e17225495d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a895c-6f5b-4173-ab86-ac985462e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "def hyperparamtune(classifier, param_grid,metric,verbose_value,cv):\n",
    "    model=model_selection.GridSearchCV(\n",
    "            estimator=classifier,\n",
    "            param_grid=param_grid,\n",
    "            scoring=metric,\n",
    "            verbose=verbose_value,            \n",
    "            cv=cv)\n",
    "\n",
    "    model.fit(x_train_tfidf,y_train)\n",
    "    print(\"Best Score %s\" % {model.best_score_})\n",
    "    print(\"Best hyperparameter set:\")\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n",
    "    return model, best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad127296-9183-4af0-8df3-985520b3592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Define grid of hyper parameters\n",
    "param_gd={\"n_estimators\":[100,200,300],\n",
    "         \"max_depth\":[11,13,17,19,23],\n",
    "         \"criterion\":[\"gini\",\"entropy\"],\n",
    "         \"min_samples_split\":[3,7,11],\n",
    "         \"min_samples_leaf\":[3,5],\n",
    "         \"max_features\":[\"sqrt\", \"log2\"]}\n",
    "\n",
    "model_8, best_param_8 = hyperparamtune(RandomForestClassifier(),param_gd,\"accuracy\",10,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f4bb6-8dca-4d10-b9dd-b3924d084d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "model = None\n",
    "if ((f1_score_1>f1_score_2) & (f1_score_1>f1_score_3) & (f1_score_1>f1_score_4) & (f1_score_1>f1_score_5)& (f1_score_1>f1_score_7)&(f1_score_1>f1_score_8)):\n",
    "    model = model_1\n",
    "    print(\"Logsitics Regression is providing best F1 score: %f\" % f1_score_1)\n",
    "elif((f1_score_2>f1_score_1) & (f1_score_2>f1_score_3) & (f1_score_2>f1_score_4) & (f1_score_2>f1_score_5)& (f1_score_2>f1_score_7)&(f1_score_2>f1_score_8)):\n",
    "    model = model_2\n",
    "    print(\" Over fit Decision Tree is providing best F1 score: %f\" % f1_score_2)\n",
    "elif((f1_score_3>f1_score_1) & (f1_score_3>f1_score_2) & (f1_score_3>f1_score_4)&(f1_score_3>f1_score_5)& (f1_score_3>f1_score_7)&(f1_score_3>f1_score_8)):\n",
    "    model = model_3\n",
    "    print(\"Decision Tree is providing best F1 score: %f\" % f1_score_3)\n",
    "elif((f1_score_4>f1_score_1) & (f1_score_4>f1_score_2) & (f1_score_4>f1_score_3)&(f1_score_4>f1_score_5)& (f1_score_4>f1_score_7)&(f1_score_4>f1_score_8)):\n",
    "    model = model_4\n",
    "    print(\"Random Forest is providing best F1 score: %f\" % f1_score_4)\n",
    "elif((f1_score_5>f1_score_1) & (f1_score_5>f1_score_2) & (f1_score_5>f1_score_4)&(f1_score_5>f1_score_3)& (f1_score_5>f1_score_7)&(f1_score_5>f1_score_8)):\n",
    "    model = model_5\n",
    "    print(\"Adaboost Classifier is providing best F1 score: %f\" % f1_score_5)\n",
    "elif((f1_score_7>f1_score_1) & (f1_score_7>f1_score_2) & (f1_score_7>f1_score_4)&(f1_score_7>f1_score_3)& (f1_score_7>f1_score_5)&(f1_score_7>f1_score_8)):\n",
    "    model = model_7\n",
    "    print(\"Finetuned Logsitics Regression Classifier is providing best F1 score: %f\" % f1_score_7)\n",
    "elif((f1_score_8>f1_score_1) & (f1_score_8>f1_score_2) & (f1_score_8>f1_score_4)&(f1_score_8>f1_score_3)& (f1_score_8>f1_score_7)&(f1_score_8>f1_score_5)):\n",
    "    model = model_8\n",
    "    print(\"Finetuned Random Forest Classifier is providing best F1 score: %f\" % f1_score_8)\n",
    "else:\n",
    "    print(\"No Model is selected, Train again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd0d69-1f18-4b27-af23-edf780750e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba7c47-1a88-4570-bf9c-8f65d4a68926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01448e56-55da-4c10-a1b8-f888606b7230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
